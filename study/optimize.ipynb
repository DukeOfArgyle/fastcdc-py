{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastcdc-py - optimization study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import os\n",
    "from codetiming import Timer\n",
    "import humanize\n",
    "import cython\n",
    "import random\n",
    "from array import array\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 512)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_block_size_win():\n",
    "    \"\"\"Check storage cluster/sector sizes\"\"\"\n",
    "    sectorsPerCluster = ctypes.c_ulonglong(0)\n",
    "    bytesPerSector = ctypes.c_ulonglong(0)\n",
    "    rootPathName = ctypes.c_wchar_p(u\"C:\\\\\")\n",
    "\n",
    "    ctypes.windll.kernel32.GetDiskFreeSpaceW(rootPathName,\n",
    "        ctypes.pointer(sectorsPerCluster),\n",
    "        ctypes.pointer(bytesPerSector),\n",
    "        None,\n",
    "        None,\n",
    "    )\n",
    "    return sectorsPerCluster.value, bytesPerSector.value\n",
    "get_block_size_win()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate 1.0 GB dummy.bin\n"
     ]
    }
   ],
   "source": [
    "DUMMY=\"dummy.bin\"\n",
    "\n",
    "def create_dummy_file(size=int(1e+9)):\n",
    "    \"\"\"Create a Random Data File\"\"\"\n",
    "    print(f'Generate {humanize.naturalsize(size)} {DUMMY}')\n",
    "    with open(DUMMY, \"wb\") as outf:\n",
    "        outf.write(os.urandom(size))\n",
    "\n",
    "create_dummy_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_read(func, chunk_size):\n",
    "    func_name = func.__name__\n",
    "    num_bytes = os.path.getsize(DUMMY)\n",
    "    file_size = humanize.naturalsize(num_bytes)\n",
    "    t = Timer(logger=None)\n",
    "    t.start()\n",
    "    result = func(chunk_size)\n",
    "    t.stop()\n",
    "    print(f\"{file_size} with {humanize.naturalsize(chunk_size)} chunk size with {func_name} : {humanize.naturalsize(num_bytes / t.last)}/s (result = {result})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunked(chunk_size=1024*32):\n",
    "    with open(DUMMY, 'rb') as f:\n",
    "        data = f.read(chunk_size)\n",
    "        x = None\n",
    "        while data:\n",
    "            x = data\n",
    "            data = f.read(chunk_size)\n",
    "    return x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 GB with 1.0 kB chunk size with read_chunked : 1.5 GB/s (result = 46)\n",
      "1.0 GB with 8.2 kB chunk size with read_chunked : 1.9 GB/s (result = 46)\n",
      "1.0 GB with 16.4 kB chunk size with read_chunked : 2.9 GB/s (result = 46)\n",
      "1.0 GB with 32.8 kB chunk size with read_chunked : 4.3 GB/s (result = 46)\n",
      "1.0 GB with 65.5 kB chunk size with read_chunked : 5.3 GB/s (result = 46)\n",
      "1.0 GB with 131.1 kB chunk size with read_chunked : 6.1 GB/s (result = 46)\n",
      "1.0 GB with 262.1 kB chunk size with read_chunked : 6.8 GB/s (result = 46)\n",
      "1.0 GB with 524.3 kB chunk size with read_chunked : 7.1 GB/s (result = 46)\n",
      "1.0 GB with 1.0 MB chunk size with read_chunked : 2.7 GB/s (result = 46)\n"
     ]
    }
   ],
   "source": [
    "benchmark_read(read_chunked, chunk_size=1024)\n",
    "benchmark_read(read_chunked, chunk_size=1024*8)\n",
    "benchmark_read(read_chunked, chunk_size=1024*16)\n",
    "benchmark_read(read_chunked, chunk_size=1024*32)\n",
    "benchmark_read(read_chunked, chunk_size=1024*64)\n",
    "benchmark_read(read_chunked, chunk_size=1024*128)\n",
    "benchmark_read(read_chunked, chunk_size=1024*256)\n",
    "benchmark_read(read_chunked, chunk_size=1024*512)\n",
    "benchmark_read(read_chunked, chunk_size=1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = os.urandom(1024*512*100)\n",
    "GEAR = [random.getrandbits(32) for _ in range(256)]\n",
    "GEAR_A = array('L', GEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_chunker(func, data, gear):\n",
    "    func_name = func.__name__\n",
    "    num_bytes = len(data)\n",
    "    data_size = humanize.naturalsize(num_bytes)\n",
    "    t = Timer(logger=None)\n",
    "    t.start()\n",
    "    result = func(data, gear)\n",
    "    t.stop()\n",
    "    print(f\"{data_size} with {func_name} : {humanize.naturalsize(num_bytes / t.last)}/s (total={t.last:.4f}s result={result})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker_simple(data, gear):\n",
    "    pattern = 0\n",
    "    mask = 32767\n",
    "    c = 0\n",
    "    for b in data:\n",
    "        pattern = (pattern >> 1) + gear[b]\n",
    "        if not pattern & mask:\n",
    "            c+=1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "cimport cython\n",
    "from libc.stdint cimport uint32_t, uint8_t\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def chunker_cython(const uint8_t[:] data, const uint32_t[:] gear):\n",
    "    cdef uint32_t pattern, mask, c, i, length\n",
    "    pattern = 0\n",
    "    mask = 32767\n",
    "    c = 0\n",
    "    length = data.shape[0]\n",
    "    with nogil:\n",
    "        for i in range(length):\n",
    "            pattern = (pattern >> 1) + gear[data[i]]\n",
    "            if not pattern & mask:\n",
    "                c += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True)\n",
    "def chunker_numba(data, gear):\n",
    "    pattern = 0\n",
    "    mask = 32767\n",
    "    c = 0\n",
    "    for b in data:\n",
    "        pattern = (pattern >> 1) + gear[b]\n",
    "        if not pattern & mask:\n",
    "            c+=1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.4 MB with chunker_simple : 7.6 MB/s (total=6.9281s result=1656)\n",
      "52.4 MB with chunker_cython : 1.5 GB/s (total=0.0352s result=1622)\n",
      "52.4 MB with chunker_numba : 1.7 GB/s (total=0.0313s result=1656)\n"
     ]
    }
   ],
   "source": [
    "benchmark_chunker(chunker_simple, DATA, GEAR)\n",
    "benchmark_chunker(chunker_cython, DATA, GEAR_A)\n",
    "benchmark_chunker(chunker_numba, DATA, GEAR_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_multi_chunker(func):\n",
    "    func_name = func.__name__\n",
    "    num_bytes = os.path.getsize(DUMMY)    \n",
    "    data_size = humanize.naturalsize(num_bytes)\n",
    "    t = Timer(logger=None)\n",
    "    t.start()\n",
    "    result = func()\n",
    "    t.stop()\n",
    "    print(f\"{data_size} with {func_name} : {humanize.naturalsize(num_bytes / t.last)}/s (total={t.last:.4f}s result={result[:3]}...{result[-3:]})\")\n",
    "\n",
    "def fixed_chunks(chunk_size=1024*512):\n",
    "    with open(DUMMY, 'rb') as f:\n",
    "        data = f.read(chunk_size)\n",
    "        while data:\n",
    "            yield data\n",
    "            data = f.read(chunk_size)\n",
    "            \n",
    "def serial_chunker():\n",
    "    result = []\n",
    "    for chunk in fixed_chunks():\n",
    "        result.append(chunker_cython(chunk, GEAR_A))\n",
    "    return result\n",
    "\n",
    "def parallel_chunker():\n",
    "    with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "        jobs = [ex.submit(chunker_cython, chunk, GEAR_A) for chunk in fixed_chunks()]\n",
    "    return [job.result() for job in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 GB with serial_chunker : 1.2 GB/s (total=0.8270s result=[15, 15, 19]...[13, 17, 4])\n",
      "1.0 GB with parallel_chunker : 4.3 GB/s (total=0.2345s result=[15, 15, 19]...[13, 17, 4])\n"
     ]
    }
   ],
   "source": [
    "benchmark_multi_chunker(serial_chunker)\n",
    "benchmark_multi_chunker(parallel_chunker)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcdc-py",
   "language": "python",
   "name": "fastcdc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
